<!DOCTYPE html>
<html lang="en" data-dark="false">
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <!--
  put your analytics (e.g. Google Analytics) tracking code here
-->

  <!--
  put your search engine verification (e.g. Google Search Console) tag here
-->

  






























<meta name="viewport" content="width=device-width, initial-scale=1">

<title>研究 | EIT-NLP</title>

<link rel="icon" href="/lab-website/images/lab_logo_icon.png">

<meta name="title" content="研究">
<meta name="description" content="An engaging 1-3 sentence description of your lab.">

<meta property="og:title" content="研究">
<meta property="og:site_title" content="EIT-NLP">
<meta property="og:description" content="An engaging 1-3 sentence description of your lab.">
<meta property="og:url" content="https://eit-nlp.github.io/lab-website">
<meta property="og:image" content="/lab-website/images/share.jpg">
<meta property="og:locale" content="en_US">

<meta property="twitter:title" content="研究">
<meta property="twitter:description" content="An engaging 1-3 sentence description of your lab.">
<meta property="twitter:url" content="https://eit-nlp.github.io/lab-website">
<meta property="twitter:card" content="summary_large_image">
<meta property="twitter:image" content="/lab-website/images/share.jpg">


  <meta property="og:type" content="website">


<script type="application/ld+json">
  {
    "@context": "https://schema.org",
    
      "@type": "WebSite",
    
    "name": "研究",
    "description": "An engaging 1-3 sentence description of your lab.",
    "headline": "研究",
    "publisher": {
      "@type": "Organization",
      "logo": { "@type": "ImageObject", "url": "/lab-website/images/lab_logo_icon.png" }
    },
    "url": "https://eit-nlp.github.io/lab-website"
  }
</script>

<link rel="alternate" type="application/rss+xml" href="https://eit-nlp.github.io/lab-website/feed.xml">

  <!-- Google Fonts -->
<!-- automatically get url from fonts used in theme file -->

<link rel="preconnect" href="https://fonts.gstatic.com">
<link href="https://fonts.googleapis.com/css2?display=swap&&family=Barlow:ital,wght@0,200;0,400;0,500;0,600;1,200;1,400;1,500;1,600&amp;family=Roboto+Mono:ital,wght@0,200;0,400;0,500;0,600;1,200;1,400;1,500;1,600" rel="stylesheet">

<!-- Font Awesome icons (load asynchronously due to size) -->

<link href="https://use.fontawesome.com/releases/v6.7.0/css/all.css" rel="stylesheet" media="none" onload="this.removeAttribute('media'); this.onload = null;">
<noscript>
  <link href="https://use.fontawesome.com/releases/v6.7.0/css/all.css" rel="stylesheet">
</noscript>

  <!-- third party styles -->
<!-- https://stylishthemes.github.io/Syntax-Themes/pygments/ -->
<link href="https://cdn.jsdelivr.net/gh/StylishThemes/Syntax-Themes/pygments/css-github/pygments-tomorrow-night-eighties.css" rel="stylesheet">

<!-- include all sass in styles folder -->


  
    <link href="/lab-website/_styles/-theme.css" rel="stylesheet">
  

  
    <link href="/lab-website/_styles/alert.css" rel="stylesheet">
  

  
    <link href="/lab-website/_styles/all.css" rel="stylesheet">
  

  
    <link href="/lab-website/_styles/anchor.css" rel="stylesheet">
  

  
    <link href="/lab-website/_styles/background.css" rel="stylesheet">
  

  
    <link href="/lab-website/_styles/body.css" rel="stylesheet">
  

  
    <link href="/lab-website/_styles/bold.css" rel="stylesheet">
  

  
    <link href="/lab-website/_styles/button.css" rel="stylesheet">
  

  
    <link href="/lab-website/_styles/card.css" rel="stylesheet">
  

  
    <link href="/lab-website/_styles/checkbox.css" rel="stylesheet">
  

  
    <link href="/lab-website/_styles/citation.css" rel="stylesheet">
  

  
    <link href="/lab-website/_styles/code.css" rel="stylesheet">
  

  
    <link href="/lab-website/_styles/cols.css" rel="stylesheet">
  

  
    <link href="/lab-website/_styles/dark-toggle.css" rel="stylesheet">
  

  
    <link href="/lab-website/_styles/details.css" rel="stylesheet">
  

  
    <link href="/lab-website/_styles/feature.css" rel="stylesheet">
  

  
    <link href="/lab-website/_styles/figure.css" rel="stylesheet">
  

  
    <link href="/lab-website/_styles/float.css" rel="stylesheet">
  

  
    <link href="/lab-website/_styles/font.css" rel="stylesheet">
  

  
    <link href="/lab-website/_styles/footer.css" rel="stylesheet">
  

  
    <link href="/lab-website/_styles/form.css" rel="stylesheet">
  

  
    <link href="/lab-website/_styles/grid.css" rel="stylesheet">
  

  
    <link href="/lab-website/_styles/header.css" rel="stylesheet">
  

  
    <link href="/lab-website/_styles/heading.css" rel="stylesheet">
  

  
    <link href="/lab-website/_styles/highlight.css" rel="stylesheet">
  

  
    <link href="/lab-website/_styles/icon.css" rel="stylesheet">
  

  
    <link href="/lab-website/_styles/image.css" rel="stylesheet">
  

  
    <link href="/lab-website/_styles/link.css" rel="stylesheet">
  

  
    <link href="/lab-website/_styles/list.css" rel="stylesheet">
  

  
    <link href="/lab-website/_styles/main.css" rel="stylesheet">
  

  
    <link href="/lab-website/_styles/paragraph.css" rel="stylesheet">
  

  
    <link href="/lab-website/_styles/portrait.css" rel="stylesheet">
  

  
    <link href="/lab-website/_styles/post-excerpt.css" rel="stylesheet">
  

  
    <link href="/lab-website/_styles/post-info.css" rel="stylesheet">
  

  
    <link href="/lab-website/_styles/post-nav.css" rel="stylesheet">
  

  
    <link href="/lab-website/_styles/quote.css" rel="stylesheet">
  

  
    <link href="/lab-website/_styles/rule.css" rel="stylesheet">
  

  
    <link href="/lab-website/_styles/search-box.css" rel="stylesheet">
  

  
    <link href="/lab-website/_styles/search-info.css" rel="stylesheet">
  

  
    <link href="/lab-website/_styles/section.css" rel="stylesheet">
  

  
    <link href="/lab-website/_styles/table.css" rel="stylesheet">
  

  
    <link href="/lab-website/_styles/tags.css" rel="stylesheet">
  

  
    <link href="/lab-website/_styles/textbox.css" rel="stylesheet">
  

  
    <link href="/lab-website/_styles/tooltip.css" rel="stylesheet">
  

  
    <link href="/lab-website/_styles/util.css" rel="stylesheet">
  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  


<!-- include all css in styles folder -->



  <!-- third party scripts -->
<script src="https://unpkg.com/@popperjs/core@2" defer></script>
<script src="https://unpkg.com/tippy.js@6" defer></script>
<script src="https://unpkg.com/mark.js@8" defer></script>

<!-- include all js in scripts folder -->


  <script src="/lab-website/_scripts/anchors.js"></script>

  <script src="/lab-website/_scripts/dark-mode.js"></script>

  <script src="/lab-website/_scripts/fetch-tags.js"></script>

  <script src="/lab-website/_scripts/search.js"></script>

  <script src="/lab-website/_scripts/site-search.js"></script>

  <script src="/lab-website/_scripts/table-wrap.js"></script>

  <script src="/lab-website/_scripts/tooltip.js"></script>


</head>

  <body>
    











<header class="background" style="--image: url('/lab-website/images/background.jpg')" data-dark="true">
  <a href="/lab-website" class="home">
    
    <span class="logo">
      
      <img src="/lab-website/images/lab_logo_icon.png" alt="logo">
      
    </span>
    
  </a>

  <input class="nav-toggle" type="checkbox" aria-label="show/hide nav">

  <nav>
    
    
    
    <a href="/lab-website/" data-tooltip="主页">
      首页
    </a>
    
    
    
    <a href="/lab-website/research/" data-tooltip="研究项目与成果">
      研究
    </a>
    
    
    
    <a href="/lab-website/team/" data-tooltip="课题组成员">
      团队
    </a>
    
    
    
    <a href="/lab-website/hiring/" data-tooltip="加入我们">
      招聘
    </a>
    
    
    
    <a href="/lab-website/projects/" data-tooltip="项目">
      项目
    </a>
    
    
  </nav>
</header>
    <main>
      <!--
  modify main content of page:
  - add section breaks
  - attach section properties
  - filter out blank sections
-->






  
  
  

  <section class="background" data-size="page">
    <h2 id="我们的研究">我们的研究</h2>

<div class="citation-container">
    <div class="citation">
        
        <a href="https://arxiv.org/abs/2503.08046v3" class="citation-image" aria-label="MultiConIR: Towards Multi-Condition Information Retrieval">
            <img src="/lab-website/images/papers/2503.08046v3.png" alt="MultiConIR: Towards Multi-Condition Information Retrieval" loading="lazy" onerror="this.src = '/lab-website/images/fallback.svg'; this.onerror = null;">
        </a>
        

        <div class="citation-text">
            
            <i class="icon fa-solid fa-scroll"></i>

            <a href="https://arxiv.org/abs/2503.08046v3" class="citation-title">
                MultiConIR: Towards Multi-Condition Information Retrieval
            </a>

            <div class="citation-authors" tabindex="0">
                Xuan Lu, Sifan Liu, Bochao Yin, Yongqi Li, Xinghao Chen, Hui Su, Yaohui Jin, Wenjun Zeng, Xiaoyu Shen

            </div>

            <div class="citation-details">
                <span class="citation-publisher">EMNLP 2025 Findings</span>
                 · 
                <span class="citation-date">04 Sep 2025</span>
                 · 
                <span class="citation-id">arxiv:2503.08046v3</span>
            </div>

            
            
            <div class="citation-description">
                Multi-condition information retrieval (IR) poses a significant yet underexplored challenge. We introduce MultiConIR, a benchmark for evaluating retrieval and reranking models under nuanced multi-condition queries across five domains. The benchmark examines three dimensions—complexity robustness, relevance monotonicity, and query format sensitivity. Experiments on 15 models reveal severe degradation as query complexity grows, with failures to maintain monotonicity and high sensitivity to query style and condition placement. The strong performance of GPT-4o highlights the gap between current IR systems and advanced LLMs. Our analysis further identifies factors behind reranker deterioration and the impact of condition positioning, offering insights for advancing IR systems in complex search scenarios.

            </div>
            

            
            <div class="citation-buttons">
                
                



  <div class="button-wrapper">
    <a class="button" href="https://github.com/EIT-NLP/MultiConIR" data-tooltip="Source code" data-style="bare" aria-label="Source code">
      <i class="icon fa-solid fa-code"></i>
      
        <span>code</span>
      
    </a>
  </div>


                
                



  <div class="button-wrapper">
    <a class="button" href="https://arxiv.org/abs/2503.08046v3" data-tooltip="Website" data-style="bare" aria-label="Website">
      <i class="icon fa-solid fa-globe"></i>
      
        <span>web</span>
      
    </a>
  </div>


                
            </div>
            

            
            


  <div class="tags" data-repo="EIT-NLP/MultiConIR">
    
      <a href="/lab-website/research/?search=%22tag:%20information-retrieval%22" class="tag" data-tooltip='Show items with the tag "information-retrieval"'>
        information-retrieval
      </a>
    
      <a href="/lab-website/research/?search=%22tag:%20multi-condition-retrieval%22" class="tag" data-tooltip='Show items with the tag "multi-condition-retrieval"'>
        multi-condition-retrieval
      </a>
    
  </div>


            
            
        </div>
    </div>
</div>

<div class="citation-container">
    <div class="citation">
        
        <a href="https://arxiv.org/abs/2504.21447" class="citation-image" aria-label="Multimodal Language Models See Better When They Look Shallower">
            <img src="/lab-website/images/papers/2504.21447.png" alt="Multimodal Language Models See Better When They Look Shallower" loading="lazy" onerror="this.src = '/lab-website/images/fallback.svg'; this.onerror = null;">
        </a>
        

        <div class="citation-text">
            
            <i class="icon fa-solid fa-scroll"></i>

            <a href="https://arxiv.org/abs/2504.21447" class="citation-title">
                Multimodal Language Models See Better When They Look Shallower
            </a>

            <div class="citation-authors" tabindex="0">
                Haoran Chen, Junyan Lin, Xinhao Chen, Yue Fan, Xin Jin, Hui Su, Jianfeng Dong, Jinlan Fu, Xiaoyu Shen

            </div>

            <div class="citation-details">
                <span class="citation-publisher">EMNLP 2025</span>
                 · 
                <span class="citation-date">30 Apr 2025</span>
                 · 
                <span class="citation-id">arxiv:2504.21447</span>
            </div>

            
            
            <div class="citation-description">
                Multimodal large language models (MLLMs) have achieved impressive performance across a wide range of tasks, typically using CLIP-ViT as their visual encoder due to its strong text-image alignment capabilities. While prior studies suggest that different CLIP-ViT layers capture different types of information, with shallower layers focusing on fine visual details and deeper layers aligning more closely with textual semantics, most MLLMs still select visual features based on empirical heuristics rather than systematic analysis. In this work, we propose a Layer-wise Representation Similarity approach to group CLIP-ViT layers with similar behaviors into {shallow, middle, and deep} categories and assess their impact on MLLM performance. Building on this foundation, we revisit the visual layer selection problem in MLLMs at scale, training LLaVA-style models ranging from 1.4B to 7B parameters. Through extensive experiments across 10 datasets and 4 tasks, we find that: (1) deep layers are essential for OCR tasks; (2) shallow and middle layers substantially outperform deep layers on reasoning tasks involving counting, positioning, and object localization; (3) a lightweight fusion of features across shallow, middle, and deep layers consistently outperforms specialized fusion baselines and single-layer selections, achieving gains on 9 out of 10 datasets. Our work offers the first principled study of visual layer selection in MLLMs, laying the groundwork for deeper investigations into visual representation learning for MLLMs.

            </div>
            

            
            <div class="citation-buttons">
                
                



  <div class="button-wrapper">
    <a class="button" href="https://arxiv.org/abs/2504.21447" data-tooltip="Website" data-style="bare" aria-label="Website">
      <i class="icon fa-solid fa-globe"></i>
      
        <span>web</span>
      
    </a>
  </div>


                
            </div>
            

            
            


  <div class="tags">
    
      <a href="/lab-website/research/?search=%22tag:%20mllm%22" class="tag" data-tooltip='Show items with the tag "mllm"'>
        mllm
      </a>
    
      <a href="/lab-website/research/?search=%22tag:%20visual-layer-selection%22" class="tag" data-tooltip='Show items with the tag "visual-layer-selection"'>
        visual-layer-selection
      </a>
    
  </div>


            
            
        </div>
    </div>
</div>

<div class="citation-container">
    <div class="citation">
        
        <a class="citation-image" aria-label="VisiPruner: Decoding Discontinuous Cross-Modal Dynamics for Efficient Multimodal LLMs">
            <img src="/lab-website/images/papers/VisiPruner.png" alt="VisiPruner: Decoding Discontinuous Cross-Modal Dynamics for Efficient Multimodal LLMs" loading="lazy" onerror="this.src = '/lab-website/images/fallback.svg'; this.onerror = null;">
        </a>
        

        <div class="citation-text">
            
            <i class="icon fa-solid fa-scroll"></i>

            <a class="citation-title">
                VisiPruner: Decoding Discontinuous Cross-Modal Dynamics for Efficient Multimodal LLMs
            </a>

            <div class="citation-authors" tabindex="0">
                Yingqi Fan, Anhao Zhao, Jinlan Fu, Junlong Tong, Hui Su, Yijie Pan, Wei Zhang, Xiaoyu Shen

            </div>

            <div class="citation-details">
                <span class="citation-publisher">EMNLP 2025</span>
                 · 
                <span class="citation-date">04 Sep 2025</span>
                 · 
                <span class="citation-id">/</span>
            </div>

            
            
            <div class="citation-description">
                Multimodal Large Language Models (MLLMs) have achieved impressive performance on vision–language tasks, yet the quadratic growth of attention computation with respect to multimodal tokens incurs significant computational overhead. Although prior studies have explored token pruning in MLLMs, a fundamental understanding of how these models process and integrate multimodal information remains lacking. Through systematic analysis, this work reveals a three-stage process of cross-modal interaction in MLLMs: (1) in the shallow layers, the model primarily identifies task intent, with visual tokens acting as passive attention absorbers; (2) in the middle layers, cross-modal fusion occurs abruptly and is driven by a small set of key visual tokens; (3) in the deep layers, visual tokens are gradually discarded, and the model focuses solely on fine-grained linguistic processing. Building on these insights, we propose VisiPruner, a training-free dynamic pruning framework that reduces up to 99% of vision-related attention computation and 62.8% of FLOPs while preserving model performance. Moreover, VisiPruner demonstrates strong generalization across various MLLMs. Beyond pruning, our findings provide actionable guidance for training future efficient MLLMs, suggesting that aligning model architectures with their hierarchical processing dynamics can substantially improve efficiency.

            </div>
            

            

            
            


  <div class="tags">
    
      <a href="/lab-website/research/?search=%22tag:%20mllm%22" class="tag" data-tooltip='Show items with the tag "mllm"'>
        mllm
      </a>
    
      <a href="/lab-website/research/?search=%22tag:%20model-pruning%22" class="tag" data-tooltip='Show items with the tag "model-pruning"'>
        model-pruning
      </a>
    
  </div>


            
            
        </div>
    </div>
</div>

<div class="citation-container">
    <div class="citation">
        
        <a class="citation-image" aria-label="PricingLogic: Evaluating LLMs Reasoning on Complex Tourism Pricing Tasks">
            <img src="/lab-website/images/papers/PricingLogic.png" alt="PricingLogic: Evaluating LLMs Reasoning on Complex Tourism Pricing Tasks" loading="lazy" onerror="this.src = '/lab-website/images/fallback.svg'; this.onerror = null;">
        </a>
        

        <div class="citation-text">
            
            <i class="icon fa-solid fa-scroll"></i>

            <a class="citation-title">
                PricingLogic: Evaluating LLMs Reasoning on Complex Tourism Pricing Tasks
            </a>

            <div class="citation-authors" tabindex="0">
                Yunuo Liu, Zena Al Khalili, Dawei Zhu, Yanjun Chen, Dietriech Klakow, Xiaoyu Shen

            </div>

            <div class="citation-details">
                <span class="citation-publisher">EMNLP 2025</span>
                 · 
                <span class="citation-date">04 Sep 2025</span>
                 · 
                <span class="citation-id">/</span>
            </div>

            
            
            <div class="citation-description">
                We present PricingLogic, the first benchmark designed to evaluate whether Large Language Models (LLMs) can reliably automate tourism-booking prices under multiple, overlapping fare rules. Travel agencies are eager to delegate this error-prone task to AI systems; however, deploying LLMs without verified reliability risks substantial financial losses and undermines customer trust. PricingLogic consists of 300 natural-language booking requests derived from 42 real-world pricing policies, covering two levels of difficulty: (i) basic customer-type pricing and (ii) bundled-tour calculations involving interacting discounts. Evaluations across a range of LLMs reveal a sharp performance decline on the more challenging tier, exposing systematic weaknesses in rule interpretation and arithmetic reasoning. These findings underscore that, despite their broad capabilities, current LLMs remain unreliable for revenue-critical applications without additional safeguards or domain-specific adaptation.

            </div>
            

            

            
            


  <div class="tags">
    
      <a href="/lab-website/research/?search=%22tag:%20reasoning%22" class="tag" data-tooltip='Show items with the tag "reasoning"'>
        reasoning
      </a>
    
      <a href="/lab-website/research/?search=%22tag:%20pricinglogic-benchmark%22" class="tag" data-tooltip='Show items with the tag "pricinglogic-benchmark"'>
        pricinglogic-benchmark
      </a>
    
  </div>


            
            
        </div>
    </div>
</div>

<div class="citation-container">
    <div class="citation">
        
        <a href="https://arxiv.org/abs/2504.18373" class="citation-image" aria-label="Auto-SLURP: A Benchmark Dataset for Evaluating Multi-Agent Frameworks in Smart Personal Assistant">
            <img src="/lab-website/images/papers/2504.18373.png" alt="Auto-SLURP: A Benchmark Dataset for Evaluating Multi-Agent Frameworks in Smart Personal Assistant" loading="lazy" onerror="this.src = '/lab-website/images/fallback.svg'; this.onerror = null;">
        </a>
        

        <div class="citation-text">
            
            <i class="icon fa-solid fa-scroll"></i>

            <a href="https://arxiv.org/abs/2504.18373" class="citation-title">
                Auto-SLURP: A Benchmark Dataset for Evaluating Multi-Agent Frameworks in Smart Personal Assistant
            </a>

            <div class="citation-authors" tabindex="0">
                Lei Shen, Xiaoyu Shen

            </div>

            <div class="citation-details">
                <span class="citation-publisher">EMNLP 2025 Findings</span>
                 · 
                <span class="citation-date">25 Apr 2025</span>
                 · 
                <span class="citation-id">arxiv:2504.18373</span>
            </div>

            
            
            <div class="citation-description">
                In recent years, multi-agent frameworks powered by large language models (LLMs) have advanced rapidly. Despite this progress, there is still a notable absence of benchmark datasets specifically tailored to evaluate their performance. To bridge this gap, we introduce Auto-SLURP, a benchmark dataset aimed at evaluating LLM-based multi-agent frameworks in the context of intelligent personal assistants. Auto-SLURP extends the original SLURP dataset – initially developed for natural language understanding tasks – by relabeling the data and integrating simulated servers and external services. This enhancement enables a comprehensive end-to-end evaluation pipeline, covering language understanding, task execution, and response generation. Our experiments demonstrate that Auto-SLURP presents a significant challenge for current state-of-the-art frameworks, highlighting that truly reliable and intelligent multi-agent personal assistants remain a work in progress.

            </div>
            

            
            <div class="citation-buttons">
                
                



  <div class="button-wrapper">
    <a class="button" href="https://github.com/lorashen/Auto-SLURP/" data-tooltip="Source code" data-style="bare" aria-label="Source code">
      <i class="icon fa-solid fa-code"></i>
      
        <span>code</span>
      
    </a>
  </div>


                
                



  <div class="button-wrapper">
    <a class="button" href="https://arxiv.org/abs/2504.18373" data-tooltip="Website" data-style="bare" aria-label="Website">
      <i class="icon fa-solid fa-globe"></i>
      
        <span>web</span>
      
    </a>
  </div>


                
            </div>
            

            
            


  <div class="tags" data-repo="lorashen/Auto-SLURP">
    
      <a href="/lab-website/research/?search=%22tag:%20multi-agent-framework%22" class="tag" data-tooltip='Show items with the tag "multi-agent-framework"'>
        multi-agent-framework
      </a>
    
  </div>


            
            
        </div>
    </div>
</div>

<div class="citation-container">
    <div class="citation">
        
        <a href="https://arxiv.org/abs/2508.01852" class="citation-image" aria-label="Context Guided Transformer Entropy Modeling for Video Compression">
            <img src="/lab-website/images/papers/2508.01852.png" alt="Context Guided Transformer Entropy Modeling for Video Compression" loading="lazy" onerror="this.src = '/lab-website/images/fallback.svg'; this.onerror = null;">
        </a>
        

        <div class="citation-text">
            
            <i class="icon fa-solid fa-scroll"></i>

            <a href="https://arxiv.org/abs/2508.01852" class="citation-title">
                Context Guided Transformer Entropy Modeling for Video Compression
            </a>

            <div class="citation-authors" tabindex="0">
                Junlong Tong, Wei Zhang, Yaohui Jin, Xiaoyu Shen

            </div>

            <div class="citation-details">
                <span class="citation-publisher">ICCV 2025</span>
                 · 
                <span class="citation-date">03 Aug 2025</span>
                 · 
                <span class="citation-id">arxiv:2508.01852</span>
            </div>

            
            
            <div class="citation-description">
                The entropy model is a critical component in video compression, as it determines the coding efficiency by estimating the probability distribution of quantized latent representations. A key challenge in entropy model lies in effectively capturing and utilizing contextual information, including temporal and spatial contexts, to accurately estimate the probability mass functions (PMFs) of video frames. This work introduces a Context Guided Transformer (CGT) entropy model, which leverages a temporal context resampler to distill long-range temporal cues into fixed-length representations and a dependency-weighted spatial context assigner to selectively model spatial dependencies in a structured order. By jointly addressing temporal and spatial context utilization, our approach achieves improved compression performance while significantly reducing computational overhead.

            </div>
            

            
            <div class="citation-buttons">
                
                



  <div class="button-wrapper">
    <a class="button" href="https://github.com/EIT-NLP/CGT" data-tooltip="Source code" data-style="bare" aria-label="Source code">
      <i class="icon fa-solid fa-code"></i>
      
        <span>code</span>
      
    </a>
  </div>


                
                



  <div class="button-wrapper">
    <a class="button" href="https://arxiv.org/abs/2508.01852" data-tooltip="Website" data-style="bare" aria-label="Website">
      <i class="icon fa-solid fa-globe"></i>
      
        <span>web</span>
      
    </a>
  </div>


                
            </div>
            

            
            


  <div class="tags" data-repo="EIT-NLP/CGT">
    
      <a href="/lab-website/research/?search=%22tag:%20video-compression%22" class="tag" data-tooltip='Show items with the tag "video-compression"'>
        video-compression
      </a>
    
  </div>


            
            
        </div>
    </div>
</div>

<div class="citation-container">
    <div class="citation">
        
        <a href="https://arxiv.org/abs/2505.16983" class="citation-image" aria-label="LLM as Effective Streaming Processor: Bridging Streaming-Batch Mismatches with Group Position Encoding">
            <img src="/lab-website/images/papers/2505.16983.png" alt="LLM as Effective Streaming Processor: Bridging Streaming-Batch Mismatches with Group Position Encoding" loading="lazy" onerror="this.src = '/lab-website/images/fallback.svg'; this.onerror = null;">
        </a>
        

        <div class="citation-text">
            
            <i class="icon fa-solid fa-scroll"></i>

            <a href="https://arxiv.org/abs/2505.16983" class="citation-title">
                LLM as Effective Streaming Processor: Bridging Streaming-Batch Mismatches with Group Position Encoding
            </a>

            <div class="citation-authors" tabindex="0">
                Junlong Tong, Jinlan Fu, Zixuan Lin, Yingqi Fan, Anhao Zhao, Hui Su, Xiaoyu Shen

            </div>

            <div class="citation-details">
                <span class="citation-publisher">EMNLP 2025 Findings</span>
                 · 
                <span class="citation-date">29 May 2025</span>
                 · 
                <span class="citation-id">arxiv:2505.16983</span>
            </div>

            
            
            <div class="citation-description">
                Large Language Models (LLMs) are primarily designed for batch processing. Existing methods for adapting LLMs to streaming rely either on expensive re-encoding or specialized architectures with limited scalability. This work identifies three key mismatches in adapting batch-oriented LLMs to streaming: (1) input-attention, (2) output-attention, and (3) position-ID mismatches. While it is commonly assumed that the latter two mismatches require frequent re-encoding, our analysis reveals that only the input-attention mismatch significantly impacts performance, indicating re-encoding outputs is largely unnecessary. To better understand this discrepancy with the common assumption, we provide the first comprehensive analysis of the impact of position encoding on LLMs in streaming, showing that preserving relative positions within source and target contexts is more critical than maintaining absolute order. Motivated by the above analysis, we introduce a group position encoding paradigm built on batch architectures to enhance consistency between streaming and batch modes. Extensive experiments on cross-lingual and cross-modal tasks demonstrate that our method outperforms existing approaches. Our method requires no architectural modifications, exhibits strong generalization in both streaming and batch modes.

            </div>
            

            
            <div class="citation-buttons">
                
                



  <div class="button-wrapper">
    <a class="button" href="https://github.com/EIT-NLP/StreamingLLM" data-tooltip="Source code" data-style="bare" aria-label="Source code">
      <i class="icon fa-solid fa-code"></i>
      
        <span>code</span>
      
    </a>
  </div>


                
                



  <div class="button-wrapper">
    <a class="button" href="https://arxiv.org/abs/2505.16983" data-tooltip="Website" data-style="bare" aria-label="Website">
      <i class="icon fa-solid fa-globe"></i>
      
        <span>web</span>
      
    </a>
  </div>


                
            </div>
            

            
            


  <div class="tags" data-repo="EIT-NLP/StreamingLLM">
    
      <a href="/lab-website/research/?search=%22tag:%20streaming-adaptation%22" class="tag" data-tooltip='Show items with the tag "streaming-adaptation"'>
        streaming-adaptation
      </a>
    
      <a href="/lab-website/research/?search=%22tag:%20group-streaming-paradigm%22" class="tag" data-tooltip='Show items with the tag "group-streaming-paradigm"'>
        group-streaming-paradigm
      </a>
    
  </div>


            
            
        </div>
    </div>
</div>

<div class="citation-container">
    <div class="citation">
        
        <a href="https://arxiv.org/abs/2506.04179" class="citation-image" aria-label="SkipGPT: Each Token is One of a Kind">
            <img src="/lab-website/images/papers/2506.04179.png" alt="SkipGPT: Each Token is One of a Kind" loading="lazy" onerror="this.src = '/lab-website/images/fallback.svg'; this.onerror = null;">
        </a>
        

        <div class="citation-text">
            
            <i class="icon fa-solid fa-scroll"></i>

            <a href="https://arxiv.org/abs/2506.04179" class="citation-title">
                SkipGPT: Each Token is One of a Kind
            </a>

            <div class="citation-authors" tabindex="0">
                Anhao Zhao, Fanghua Ye, Yingqi Fan, Junlong Tong, Zhiwei Fei, Hui Su, Xiaoyu Shen

            </div>

            <div class="citation-details">
                <span class="citation-publisher">ICML 2025</span>
                 · 
                <span class="citation-date">04 Jan 2025</span>
                 · 
                <span class="citation-id">arxiv:2506.04179</span>
            </div>

            
            
            <div class="citation-description">
                Large Language Models (LLMs) deliver impressive performance but at high computational cost. SkipGPT introduces a dynamic layer pruning framework that improves efficiency by combining token-aware routing with differentiated pruning strategies for MLP and attention layers. To ensure stability, it adopts a two-stage optimization process: learning routing policies first, then restoring performance through lightweight fine-tuning. Experiments show SkipGPT reduces parameters by over 40% while maintaining or surpassing the original model’s accuracy, offering a practical path toward scalable and resource-efficient LLM deployment.

            </div>
            

            
            <div class="citation-buttons">
                
                



  <div class="button-wrapper">
    <a class="button" href="https://github.com/EIT-NLP/SkipGPT" data-tooltip="Source code" data-style="bare" aria-label="Source code">
      <i class="icon fa-solid fa-code"></i>
      
        <span>code</span>
      
    </a>
  </div>


                
                



  <div class="button-wrapper">
    <a class="button" href="https://arxiv.org/abs/2506.04179" data-tooltip="Website" data-style="bare" aria-label="Website">
      <i class="icon fa-solid fa-globe"></i>
      
        <span>web</span>
      
    </a>
  </div>


                
            </div>
            

            
            


  <div class="tags" data-repo="EIT-NLP/SkipGPT">
    
      <a href="/lab-website/research/?search=%22tag:%20model-efficiency%22" class="tag" data-tooltip='Show items with the tag "model-efficiency"'>
        model-efficiency
      </a>
    
      <a href="/lab-website/research/?search=%22tag:%20dynamic-layer-pruning%22" class="tag" data-tooltip='Show items with the tag "dynamic-layer-pruning"'>
        dynamic-layer-pruning
      </a>
    
  </div>


            
            
        </div>
    </div>
</div>

<div class="citation-container">
    <div class="citation">
        
        <a href="https://arxiv.org/abs/2504.07858" class="citation-image" aria-label="Scaling Under-Resourced TTS: A Data-Optimized Framework with Advanced Acoustic Modeling for Thai">
            <img src="/lab-website/images/papers/2504.07858.png" alt="Scaling Under-Resourced TTS: A Data-Optimized Framework with Advanced Acoustic Modeling for Thai" loading="lazy" onerror="this.src = '/lab-website/images/fallback.svg'; this.onerror = null;">
        </a>
        

        <div class="citation-text">
            
            <i class="icon fa-solid fa-scroll"></i>

            <a href="https://arxiv.org/abs/2504.07858" class="citation-title">
                Scaling Under-Resourced TTS: A Data-Optimized Framework with Advanced Acoustic Modeling for Thai
            </a>

            <div class="citation-authors" tabindex="0">
                Yizhong Geng, Jizhuo Xu, Zeyu Liang, Jinghan Yang, Xiaoyi Shi, Xiaoyu Shen

            </div>

            <div class="citation-details">
                <span class="citation-publisher">ACL 2025 (Industry Track)</span>
                 · 
                <span class="citation-date">10 Apr 2025</span>
                 · 
                <span class="citation-id">arxiv:2504.07858</span>
            </div>

            
            
            <div class="citation-description">
                Text-to-speech (TTS) technology has achieved impressive results for widely spoken languages, yet many under-resourced languages remain challenged by limited data and linguistic complexities. In this paper, we present a novel methodology that integrates a data-optimized framework with an advanced acoustic model to build high-quality TTS systems for low-resource scenarios. We demonstrate the effectiveness of our approach using Thai as an illustrative case, where intricate phonetic rules and sparse resources are effectively addressed. Our method enables zero-shot voice cloning and improved performance across diverse client applications, ranging from finance to healthcare, education, and law. Extensive evaluations—both subjective and objective—confirm that our model meets state-of-the-art standards, offering a scalable solution for TTS production in data-limited settings, with significant implications for broader industry adoption and multilingual accessibility.

            </div>
            

            
            <div class="citation-buttons">
                
                



  <div class="button-wrapper">
    <a class="button" href="https://luoji.cn/static/thai/demo.html" data-tooltip="Source code" data-style="bare" aria-label="Source code">
      <i class="icon fa-solid fa-code"></i>
      
        <span>演示页面</span>
      
    </a>
  </div>


                
                



  <div class="button-wrapper">
    <a class="button" href="https://arxiv.org/abs/2504.07858" data-tooltip="Website" data-style="bare" aria-label="Website">
      <i class="icon fa-solid fa-globe"></i>
      
        <span>web</span>
      
    </a>
  </div>


                
            </div>
            

            
            


  <div class="tags">
    
      <a href="/lab-website/research/?search=%22tag:%20text-to-speech%22" class="tag" data-tooltip='Show items with the tag "text-to-speech"'>
        text-to-speech
      </a>
    
      <a href="/lab-website/research/?search=%22tag:%20low-resource-languages%22" class="tag" data-tooltip='Show items with the tag "low-resource-languages"'>
        low-resource-languages
      </a>
    
  </div>


            
            
        </div>
    </div>
</div>

<div class="citation-container">
    <div class="citation">
        
        <a href="https://arxiv.org/abs/2502.18001" class="citation-image" aria-label="Unveiling the Key Factors for Distilling Chain-of-Thought Reasoning">
            <img src="/lab-website/images/papers/2502.18001.png" alt="Unveiling the Key Factors for Distilling Chain-of-Thought Reasoning" loading="lazy" onerror="this.src = '/lab-website/images/fallback.svg'; this.onerror = null;">
        </a>
        

        <div class="citation-text">
            
            <i class="icon fa-solid fa-scroll"></i>

            <a href="https://arxiv.org/abs/2502.18001" class="citation-title">
                Unveiling the Key Factors for Distilling Chain-of-Thought Reasoning
            </a>

            <div class="citation-authors" data-tooltip="Xinghao Chen, Zhijing Sun, Wenjin Guo, Miaoran Zhang, Yanjun Chen, Yirong Sun, Hui Su, Yijie Pan, Dietrich Klakow, Wenjie Li, Xiaoyu Shen" tabindex="0">
                Xinghao Chen, Zhijing Sun, Wenjin Guo, Miaoran Zhang, Yanjun Chen, …, Hui Su, Yijie Pan, Dietrich Klakow, Wenjie Li, Xiaoyu Shen

            </div>

            <div class="citation-details">
                <span class="citation-publisher">ACL 2025 Findings</span>
                 · 
                <span class="citation-date">27 May 2025</span>
                 · 
                <span class="citation-id">arxiv:2502.18001</span>
            </div>

            
            
            <div class="citation-description">
                Large Language Models (LLMs) achieve strong reasoning via Chain-of-Thought (CoT) prompting, but at high computational cost, motivating CoT distillation into Small Language Models (SLMs). This work examines how granularity, format, and teacher choice affect CoT distillation through experiments on four teachers, seven students, and seven reasoning benchmarks. We find: (1) SLMs show a non-monotonic response to granularity—stronger models prefer finer reasoning, weaker ones simpler supervision; (2) CoT format strongly influences LLMs but barely affects SLMs; (3) Stronger teachers do not always yield better students, as diversity and complexity can outweigh accuracy. These results highlight the need to tailor CoT strategies to student models, offering practical guidance for SLM distillation.

            </div>
            

            
            <div class="citation-buttons">
                
                



  <div class="button-wrapper">
    <a class="button" href="https://github.com/EIT-NLP/Distilling-CoT-Reasoning" data-tooltip="Source code" data-style="bare" aria-label="Source code">
      <i class="icon fa-solid fa-code"></i>
      
        <span>code</span>
      
    </a>
  </div>


                
                



  <div class="button-wrapper">
    <a class="button" href="https://arxiv.org/abs/2502.18001" data-tooltip="Website" data-style="bare" aria-label="Website">
      <i class="icon fa-solid fa-globe"></i>
      
        <span>web</span>
      
    </a>
  </div>


                
            </div>
            

            
            


  <div class="tags" data-repo="EIT-NLP/Distilling-CoT-Reasoning">
    
      <a href="/lab-website/research/?search=%22tag:%20cot%22" class="tag" data-tooltip='Show items with the tag "cot"'>
        cot
      </a>
    
      <a href="/lab-website/research/?search=%22tag:%20distillation%22" class="tag" data-tooltip='Show items with the tag "distillation"'>
        distillation
      </a>
    
  </div>


            
            
        </div>
    </div>
</div>

<div class="citation-container">
    <div class="citation">
        
        <a href="https://arxiv.org/abs/2503.06063" class="citation-image" aria-label="Multi-Layer Visual Feature Fusion in Multimodal LLMs: Methods, Analysis, and Best Practices">
            <img src="/lab-website/images/papers/2503.06063.png" alt="Multi-Layer Visual Feature Fusion in Multimodal LLMs: Methods, Analysis, and Best Practices" loading="lazy" onerror="this.src = '/lab-website/images/fallback.svg'; this.onerror = null;">
        </a>
        

        <div class="citation-text">
            
            <i class="icon fa-solid fa-scroll"></i>

            <a href="https://arxiv.org/abs/2503.06063" class="citation-title">
                Multi-Layer Visual Feature Fusion in Multimodal LLMs: Methods, Analysis, and Best Practices
            </a>

            <div class="citation-authors" tabindex="0">
                Junyan Lin, Haoran Chen, Yue Fan, Yingqi Fan, Xin Jin, Hui Su, Jinlan Fu, Xiaoyu Shen

            </div>

            <div class="citation-details">
                <span class="citation-publisher">CVPR 2025</span>
                 · 
                <span class="citation-date">08 Mar 2025</span>
                 · 
                <span class="citation-id">arxiv:2503.06063</span>
            </div>

            
            
            <div class="citation-description">
                Multimodal Large Language Models (MLLMs) have made significant advancements in recent years, with visual features playing an increasingly critical role in enhancing model performance. However, the integration of multi-layer visual features in MLLMs remains underexplored, particularly with regard to optimal layer selection and fusion strategies. Existing methods often rely on arbitrary design choices, leading to suboptimal outcomes. In this paper, we systematically investigate two core aspects of multi-layer visual feature fusion: (1) selecting the most effective visual layers and (2) identifying the best fusion approach with the language model. Our experiments reveal that while combining visual features from multiple stages improves generalization, incorporating additional features from the same stage typically leads to diminished performance. Furthermore, we find that direct fusion of multi-layer visual features at the input stage consistently yields superior and more stable performance across various configurations.

            </div>
            

            
            <div class="citation-buttons">
                
                



  <div class="button-wrapper">
    <a class="button" href="https://github.com/EIT-NLP/Layer_Select_Fuse_for_MLLM" data-tooltip="Source code" data-style="bare" aria-label="Source code">
      <i class="icon fa-solid fa-code"></i>
      
        <span>code</span>
      
    </a>
  </div>


                
                



  <div class="button-wrapper">
    <a class="button" href="https://arxiv.org/abs/2503.06063" data-tooltip="Website" data-style="bare" aria-label="Website">
      <i class="icon fa-solid fa-globe"></i>
      
        <span>web</span>
      
    </a>
  </div>


                
            </div>
            

            
            


  <div class="tags" data-repo="EIT-NLP/Layer_Select_Fuse_for_MLLM">
    
      <a href="/lab-website/research/?search=%22tag:%20mllm%22" class="tag" data-tooltip='Show items with the tag "mllm"'>
        mllm
      </a>
    
      <a href="/lab-website/research/?search=%22tag:%20layer-selection%22" class="tag" data-tooltip='Show items with the tag "layer-selection"'>
        layer-selection
      </a>
    
  </div>


            
            
        </div>
    </div>
</div>

<div class="citation-container">
    <div class="citation">
        
        <a href="https://arxiv.org/abs/2410.23841" class="citation-image" aria-label="Beyond Content Relevance: Evaluating Instruction Following in Retrieval Models">
            <img src="/lab-website/images/papers/2410.23841.png" alt="Beyond Content Relevance: Evaluating Instruction Following in Retrieval Models" loading="lazy" onerror="this.src = '/lab-website/images/fallback.svg'; this.onerror = null;">
        </a>
        

        <div class="citation-text">
            
            <i class="icon fa-solid fa-scroll"></i>

            <a href="https://arxiv.org/abs/2410.23841" class="citation-title">
                Beyond Content Relevance: Evaluating Instruction Following in Retrieval Models
            </a>

            <div class="citation-authors" tabindex="0">
                Jianqun Zhou, Yuanlei Zheng, Wei Chen, Qianqian Zheng, Hui Su, Wei Zhang, Rui Meng, Xiaoyu Shen

            </div>

            <div class="citation-details">
                <span class="citation-publisher">ICLR 2025</span>
                 · 
                <span class="citation-date">05 Mar 2025</span>
                 · 
                <span class="citation-id">arxiv:2410.23841</span>
            </div>

            
            
            <div class="citation-description">
                Instruction-following capabilities in LLMs have progressed significantly, enabling more complex user interactions through detailed prompts. However, retrieval systems have not matched these advances, most of them still relies on traditional lexical and semantic matching techniques that fail to fully capture user intent. Recent efforts have introduced instruction-aware retrieval models, but these primarily focus on intrinsic content relevance, which neglects the importance of customized preferences for broader document-level attributes. This study evaluates the instruction-following capabilities of various retrieval models beyond content relevance, including LLM-based dense retrieval and reranking models. We develop InfoSearch, a novel retrieval evaluation benchmark spanning six document-level attributes: Audience, Keyword, Format, Language, Length, and Source, and introduce novel metrics – Strict Instruction Compliance Ratio (SICR) and Weighted Instruction Sensitivity Evaluation (WISE) to accurately assess the models’ responsiveness to instructions. Our findings indicate that although fine-tuning models on instruction-aware retrieval datasets and increasing model size enhance performance, most models still fall short of instruction compliance.

            </div>
            

            
            <div class="citation-buttons">
                
                



  <div class="button-wrapper">
    <a class="button" href="https://github.com/EIT-NLP/InfoSearch" data-tooltip="Source code" data-style="bare" aria-label="Source code">
      <i class="icon fa-solid fa-code"></i>
      
        <span>code</span>
      
    </a>
  </div>


                
                



  <div class="button-wrapper">
    <a class="button" href="https://arxiv.org/abs/2410.23841" data-tooltip="Website" data-style="bare" aria-label="Website">
      <i class="icon fa-solid fa-globe"></i>
      
        <span>web</span>
      
    </a>
  </div>


                
            </div>
            

            
            


  <div class="tags" data-repo="EIT-NLP/InfoSearch">
    
      <a href="/lab-website/research/?search=%22tag:%20information-retrieval%22" class="tag" data-tooltip='Show items with the tag "information-retrieval"'>
        information-retrieval
      </a>
    
      <a href="/lab-website/research/?search=%22tag:%20instruction-following-retrieval%22" class="tag" data-tooltip='Show items with the tag "instruction-following-retrieval"'>
        instruction-following-retrieval
      </a>
    
  </div>


            
            
        </div>
    </div>
</div>

<!--
  </section>

  
  
  

  <section
    class="background"
    data-size="page"
    
    
  >
    <!--
  <background></background>
  <dark></dark>
  <size></size>
-->
<p>–&gt;</p>

<!-- ## 更多项目







  

  

  
    

     
<div class="card" data-style="">
  <a
    
      href="https://github.com/EIT-NLP/Connector-Selection-for-MLLM"
    
    aria-label="多模态大语言模型连接器选择研究"
    class="card-image"
  >
    <img
      src="/lab-website/images/citations/2404_14122.png"
      alt="多模态大语言模型连接器选择研究"
      loading="lazy"
      onerror="this.src = '/lab-website/images/fallback.svg'; this.onerror = null;"

    >
  </a>

  <div class="card-text">
    
      <a
        
          href="https://github.com/EIT-NLP/Connector-Selection-for-MLLM"
        
        
        class="card-title"
      >
        多模态大语言模型连接器选择研究
    </a>
    

    
      <span class="card-subtitle">To Preserve or To Compress: An In-Depth Study of Connector Selection in Multimodal Large Language Models</span>
    

    
      <p>
        系统探讨连接器对多模态大语言模型性能的影响，为MLLM架构设计提供重要指导

      </p>
    

    
      


  <div
    class="tags"
    
      data-repo="EIT-NLP/Connector-Selection-for-MLLM"
    
  >
    
      <a
        href="/lab-website/research/?search=&quot;tag: 多模态学习&quot;"
        class="tag"
        data-tooltip="Show items with the tag &quot;多模态学习&quot;"
      >
        多模态学习
      </a>
    
      <a
        href="/lab-website/research/?search=&quot;tag: 大语言模型&quot;"
        class="tag"
        data-tooltip="Show items with the tag &quot;大语言模型&quot;"
      >
        大语言模型
      </a>
    
      <a
        href="/lab-website/research/?search=&quot;tag: 架构设计&quot;"
        class="tag"
        data-tooltip="Show items with the tag &quot;架构设计&quot;"
      >
        架构设计
      </a>
    
  </div>


    
  </div>
</div>

  
    

     
<div class="card" data-style="">
  <a
    
      href="https://github.com/uds-lsv/mt-sft"
    
    aria-label="大语言模型翻译微调研究"
    class="card-image"
  >
    <img
      src="/lab-website/images/projects/llm-translation.jpg"
      alt="大语言模型翻译微调研究"
      loading="lazy"
      onerror="this.src = '/lab-website/images/fallback.svg'; this.onerror = null;"

    >
  </a>

  <div class="card-text">
    
      <a
        
          href="https://github.com/uds-lsv/mt-sft"
        
        
        class="card-title"
      >
        大语言模型翻译微调研究
    </a>
    

    
      <span class="card-subtitle">Fine-Tuning Large Language Models to Translate</span>
    

    
      <p>
        重新审视多语言机器翻译成功的关键因素，发现仅需32对平行句子即可实现强大翻译能力

      </p>
    

    
      


  <div
    class="tags"
    
      data-repo="uds-lsv/mt-sft"
    
  >
    
      <a
        href="/lab-website/research/?search=&quot;tag: 机器翻译&quot;"
        class="tag"
        data-tooltip="Show items with the tag &quot;机器翻译&quot;"
      >
        机器翻译
      </a>
    
      <a
        href="/lab-website/research/?search=&quot;tag: 大语言模型&quot;"
        class="tag"
        data-tooltip="Show items with the tag &quot;大语言模型&quot;"
      >
        大语言模型
      </a>
    
      <a
        href="/lab-website/research/?search=&quot;tag: 微调&quot;"
        class="tag"
        data-tooltip="Show items with the tag &quot;微调&quot;"
      >
        微调
      </a>
    
  </div>


    
  </div>
</div>

  
    

     
<div class="card" data-style="">
  <a
    
      href="https://github.com/EIT-NLP/AccuracyParadox-RLHF"
    
    aria-label="RLHF中的准确性悖论"
    class="card-image"
  >
    <img
      src="/lab-website/images/projects/rlhf-paradox.jpg"
      alt="RLHF中的准确性悖论"
      loading="lazy"
      onerror="this.src = '/lab-website/images/fallback.svg'; this.onerror = null;"

    >
  </a>

  <div class="card-text">
    
      <a
        
          href="https://github.com/EIT-NLP/AccuracyParadox-RLHF"
        
        
        class="card-title"
      >
        RLHF中的准确性悖论
    </a>
    

    
      <span class="card-subtitle">The Accuracy Paradox in RLHF</span>
    

    
      <p>
        揭示人类反馈强化学习中的令人惊讶的悖论现象，挑战传统认知

      </p>
    

    
      


  <div
    class="tags"
    
      data-repo="EIT-NLP/AccuracyParadox-RLHF"
    
  >
    
      <a
        href="/lab-website/research/?search=&quot;tag: 强化学习&quot;"
        class="tag"
        data-tooltip="Show items with the tag &quot;强化学习&quot;"
      >
        强化学习
      </a>
    
      <a
        href="/lab-website/research/?search=&quot;tag: 人类反馈&quot;"
        class="tag"
        data-tooltip="Show items with the tag &quot;人类反馈&quot;"
      >
        人类反馈
      </a>
    
      <a
        href="/lab-website/research/?search=&quot;tag: 奖励模型&quot;"
        class="tag"
        data-tooltip="Show items with the tag &quot;奖励模型&quot;"
      >
        奖励模型
      </a>
    
  </div>


    
  </div>
</div>

  
    

     
<div class="card" data-style="">
  <a
    
      href="https://github.com/EIT-NLP/InfoSearch"
    
    aria-label="信息搜索与检索"
    class="card-image"
  >
    <img
      src="/lab-website/images/projects/info-search.jpg"
      alt="信息搜索与检索"
      loading="lazy"
      onerror="this.src = '/lab-website/images/fallback.svg'; this.onerror = null;"

    >
  </a>

  <div class="card-text">
    
      <a
        
          href="https://github.com/EIT-NLP/InfoSearch"
        
        
        class="card-title"
      >
        信息搜索与检索
    </a>
    

    
      <span class="card-subtitle">Information Search and Retrieval</span>
    

    
      <p>
        开发高效的信息检索算法和系统

      </p>
    

    
      


  <div
    class="tags"
    
      data-repo="EIT-NLP/InfoSearch"
    
  >
    
      <a
        href="/lab-website/research/?search=&quot;tag: 信息检索&quot;"
        class="tag"
        data-tooltip="Show items with the tag &quot;信息检索&quot;"
      >
        信息检索
      </a>
    
      <a
        href="/lab-website/research/?search=&quot;tag: 搜索算法&quot;"
        class="tag"
        data-tooltip="Show items with the tag &quot;搜索算法&quot;"
      >
        搜索算法
      </a>
    
  </div>


    
  </div>
</div>

  

 -->
  </section>


    </main>
    


<footer class="background" style="--image: url('/lab-website/images/background.jpg')" data-dark="true" data-size="wide">
  <!--
    <div>
      Extra details like contact info or address
    </div>
  -->

  <div>
    
      
      
      



  <div class="button-wrapper">
    <a class="button" href="https://github.com/eit-nlp" data-tooltip="GitHub" data-style="bare" aria-label="GitHub">
      <i class="icon fa-brands fa-github"></i>
      
    </a>
  </div>


    
  </div>

  <div>
    © 2025
    EIT-NLP
      |   Built with
    <a href="https://github.com/greenelab/lab-website-template">
      Lab Website Template
    </a>
  </div>

  <input type="checkbox" class="dark-toggle" data-tooltip="Dark mode" aria-label="toggle dark mode" oninput="onDarkToggleChange(event)">
</footer>

  </body>
</html>
